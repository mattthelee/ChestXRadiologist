{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_files  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing import image     \n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Flatten, Dense\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "import os\n",
    "import subprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "- Accuracy is good but no better than guess all one class. Think this could be solved by addressing class imbalance\n",
    "- Accuracy is only good if we take the binary crossentropy and not the full label accuracy. Will need to speak to the lecturer about how to measure performance for this type of multilabel data. -> suggested splitting into sublabels and report average accuracy of models vs each of the different single label classification tasks. \n",
    "\n",
    "# TODO\n",
    "- Need to balance the classes before passing them into the model. I.e. we need to take in more data to get a 50 50 split between having a disease and not, then run through the model. This should be possible as currently we're only processing 1% of the data. 10% is without any disease so that;s 20k. We then use another 20k with a disease. \n",
    "- Also need to add the gender and age into the x train so the model can use this information as well as the image. \n",
    "- May want to pass the data into a high res image generator or use the high res images, which would require using the GPU servers\n",
    "- To allow a more complex model to learn quickly on the gpu servers, may want to try using transfer learning from an existing model\n",
    "\n",
    "# Done\n",
    "- Need to change all the unknowns into positives as evidenced by the success of u-ones model on this paper: https://arxiv.org/pdf/1901.07031.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf = pd.read_csv('CheXpert-v1.0-small/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove anomalous dataline\n",
    "trainDf = trainDf[trainDf.Sex != 'Unknown']\n",
    "# Drop this column as it has many more classifications than lit suggests and shouldn't matter greatly for a CNN\n",
    "# TODO try with and without this column\n",
    "#trainDf = trainDf.drop('AP/PA', 1)\n",
    "\n",
    "def pathToID(path):\n",
    "    pathList = path.split('/')\n",
    "    return pathList[2][7:]\n",
    "\n",
    "def pathToStudy(path):\n",
    "    pathList = path.split('/')\n",
    "    return pathList[3][5:]\n",
    "\n",
    "# Convert all labels to a series of one-hot encoded labels. \n",
    "# -1 is uncertain, 0 is negative, 1 is positive, nans are no mention of the disease in the text\n",
    "trainDf = trainDf.fillna(0)\n",
    "# N.B. this is replacing unknowns with true as per u-ones model here: https://arxiv.org/pdf/1901.07031.pdf\n",
    "# This is essentialyl saying that if we're not sure of disease we say they have it. \n",
    "# Just to be on the safeside and have better recall as we care more about recall than precision\n",
    "trainDf = trainDf.replace(-1,1) \n",
    "\n",
    "\n",
    "# Onehot encode the sex and the xray orientation\n",
    "trainDf = trainDf.replace('Male',1)\n",
    "trainDf = trainDf.replace('Female',0)\n",
    "trainDf = trainDf.replace('Frontal',1)\n",
    "trainDf = trainDf.replace('Lateral',0)\n",
    "\n",
    "trainDf =trainDf.rename(index=str, columns={\"Sex\": \"Male?\",'Frontal/Lateral' :'Frontal1/Lateral0'})\n",
    "\n",
    "\n",
    "#trainDf.insert(0,'Path', trainDf['Path'])\n",
    "trainDf['Study'] = trainDf.Path.apply(pathToStudy)\n",
    "trainDf['Patient ID'] = trainDf.Path.apply(pathToID)\n",
    "\n",
    "# Rearrange Columns\n",
    "cols = ['Patient ID', 'Study', 'Path', 'Age', 'Male?', 'Frontal1/Lateral0', 'AP/PA','No Finding',\n",
    "       'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "       'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "       'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "       'Support Devices']\n",
    "trainDf = trainDf[cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows age distribution of the data set. There are 3 0-olds and 7579 90 year olds. \n",
    "# Implies that over nineties were grouped together\n",
    "ages = trainDf['Age'].value_counts()\n",
    "plt.scatter(ages.keys(),ages.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = trainDf['Male?'].value_counts()\n",
    "plt.scatter(gender.keys(),gender.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many people have no disease?\n",
    "no_finding = trainDf['No Finding'].value_counts()\n",
    "print(no_finding)\n",
    "\n",
    "# Plot pie chart to show how much of the data is labelled with each character\n",
    "values = no_finding.values\n",
    "labels = ['Disease present','All Clear']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Data Proportions', size=20)\n",
    "plt.pie(values, labels=labels, # explode=explode,\n",
    "        autopct='%1.1f%%', shadow=False, startangle=45)\n",
    " \n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    return files\n",
    "\n",
    "\n",
    "def path_to_tensor(img_path,inputSize):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, color_mode = \"grayscale\", target_size=inputSize)\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (x, x, 1)\n",
    "    x = image.img_to_array(img)\n",
    "    data = np.asarray( img, dtype=\"int32\" )\n",
    "    # convert 2D tensor to 3D tensor with shape (1, X, x) and return 3D tensor\n",
    "    return data.reshape(1,inputSize[0],inputSize[1])\n",
    "\n",
    "def paths_to_tensor(img_paths, inputSize):\n",
    "    list_of_tensors = [path_to_tensor(img_path, inputSize) for img_path in img_paths]\n",
    "    return np.array(list_of_tensors)\n",
    "\n",
    "\n",
    "def path_to_tensor_channel_last_3colour(img_path,inputSize):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, color_mode = \"grayscale\", target_size=inputSize)\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (x, x, 1)\n",
    "    x = image.img_to_array(img)\n",
    "    data = np.asarray( img, dtype=\"int32\" )\n",
    "    # convert 2D tensor to 3D tensor with shape (X, x, 3) and return 3D tensor\n",
    "    return np.stack((data,)*3, axis=-1)\n",
    "\n",
    "\n",
    "def paths_to_tensor_channel_last_3colour(img_paths, inputSize):\n",
    "    list_of_tensors = [path_to_tensor_channel_last_3colour(img_path, inputSize) for img_path in img_paths]\n",
    "    return np.array(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSize = (224,224)\n",
    "\n",
    "sample_size =20000 # 30k is the memory limit for the server\n",
    "targetColumn = [15]\n",
    "colName = trainDf.columns.tolist()[targetColumn[0]]\n",
    "print(f\"This model will be targetting {colName} column\")\n",
    "\n",
    "\n",
    "# Create balanced dataset with 50% pos examples and 50% neg examples, only take scans from the front\n",
    "pos = trainDf[(trainDf[colName] == 1) & (trainDf['Frontal1/Lateral0'] == 1 ) & (trainDf['AP/PA'] == 'AP')]\n",
    "neg = trainDf[(trainDf['No Finding'] == 1) & (trainDf['Frontal1/Lateral0'] == 1 ) & (trainDf['AP/PA'] == 'AP')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "posSample = pos.sample(int(sample_size/2))\n",
    "negSample = neg.sample(int(sample_size/2))\n",
    "sample = pd.concat([posSample,negSample])\n",
    "x_train_paths, x_val_paths, y_train, y_val = train_test_split(sample.Path, sample[colName], stratify=sample[colName], random_state =2)\n",
    "\n",
    "\n",
    "\n",
    "# The 3 channel option is required for the denseNet and other transfer learning models\n",
    "# Single channel can be used on our models\n",
    "\n",
    "#x_train = paths_to_tensor(x_train_paths,inputSize)#.astype('float32')/255\n",
    "x_train3Channel = paths_to_tensor_channel_last_3colour(x_train_paths,inputSize)#.astype('float32')/255\n",
    "\n",
    "#y_train = trainDf.iloc[:training_no,targetColumn] # to do all labels: trainDf.iloc[:training_no,8:]\n",
    "#x_val = paths_to_tensor(x_val_paths,inputSize)#.astype('float32')/255\n",
    "x_val3Channel = paths_to_tensor_channel_last_3colour(x_val_paths,inputSize)#.astype('float32')/255\n",
    "\n",
    "#y_val = trainDf.iloc[training_no:training_no+val_no,targetColumn]\n",
    "\n",
    "# Deleting dataframes in order to save memory and avoid OOM errors. \n",
    "del trainDf\n",
    "del posSample\n",
    "del negSample\n",
    "del x_train_paths\n",
    "del x_val_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(len(y_val))\n",
    "print(x_train[0].shape)\n",
    "plt.imshow(x_train[0][0], interpolation='nearest')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLDMODEL\n",
    "\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3,3), strides=(1,1), input_shape=(1,inputSize[0],inputSize[1])))\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "model.add(Conv2D(16, (3,3)))\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(32,activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(16,activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "weightsFilePath=\"weights.best.hdf5\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLDMODEL\n",
    "'''\n",
    "checkpoint = ModelCheckpoint(weightsFilePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history = model.fit(x_train,y_train, epochs = 10, batch_size=32,  validation_data=(x_val, y_val), callbacks=[checkpoint])\n",
    "model.load_weights(weightsFilePath)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old functions\n",
    "'''\n",
    "def checkAcc(predictions, truths):\n",
    "    wrongs = 0\n",
    "    for i,prediction in enumerate(predictions):\n",
    "        truth = truths[i]\n",
    "        for j, val in enumerate(prediction):\n",
    "            if val >= 0.5 and truth[j] == 0:\n",
    "                wrongs += 1\n",
    "                # break\n",
    "            if val < 0.5 and truth[j] == 1:\n",
    "                wrongs += 1\n",
    "                # break\n",
    "    total = 41*len(predictions) # len(predictions)\n",
    "    return (total - wrongs) / total, wrongs, total\n",
    "                \n",
    "            \n",
    "#checkAcc(predictions, y_val.values)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to try:\n",
    "- Could try using class weighting to handle class imbalance rather than sampling data to have no class imbalance\n",
    "- Adding in the other data as an input to a fully connected layer. This would allow network to use gender and age in it's predictions.\n",
    "- Try using no finding for the neg samples rather than 0 for the column, that way we don't have it confuse diseases. \n",
    "\n",
    "\n",
    "All of these were tested on the fracture data with 10k samples. \n",
    "## 1st Architecture:\n",
    "Tried using densenet with untrainable layers straighinto 0.3 dropout and 1 unit sigmoid output layer\n",
    "It acheived training acc around 70% and val acc of 56% after 10 epochs but didn't seem to be clearly improving\n",
    "\n",
    "## 2nd architecture\n",
    " Tried an architecture with include_top = False and two layers of 128 units and 0.2 dropouts. \n",
    "however it never got past a training acc of around 0.5. This motivates me to increase the complexity of the model and train more of it. \n",
    "## 3rd Architecture\n",
    "Same as first, but now trying with densenet first 200 layers as untrainable but rest trainable and removing last layer so it has only one class sigmoid rather than 1000 class softmax. This hadn't been possible on my laptop efore due to memm issues but is possible on Johnny's server. By removing the last layer we reduce the number of params by 1million, but allowing trainability means the network takes longer to train as we have ~4million trainable parameters. Achieved 95% training accuracy but only 54% val accuracy. At least this means we're able to get a decent accuracy somewhere, we clearly just need to prevent overfitting. Can't increase much more than 10k dataset, so best to try other ways to prevent overfitting -> dropout layers, or reducing trainable params\n",
    "\n",
    "## 4th Architecture\n",
    "addition of dropout layer before the last dense layer and also set the untrainable to be the first 300 layers rather than 200. reduces trainable params to 2 million. This actually resulted in training accuracy reaching 98% within 4 epochs, which is a bit disappointing/confusing. val acc maxed out at 0.50240Perhaps the trainble layers require more dropout or perhaps i need to furhter reduce the trainable params.Perhaps the difficulty is that it is seeing one of the other diseases, that can look like a fracture, so perhaps the data should be weighted to be 50% fracture, 50% no finding to avoid confusion -> however this is not how the model might be used in future so might not be a valid approach. perhaps it needs a higher weighting of the no-finding to achieve something similar but still training it to distinguish between fracture and other diseases. \n",
    "\n",
    "## 5th Architecture\n",
    "To avoid the overfitting, we reduce the number of trainable layers, so the first 400 layers are untrainable. Results in a mere 575K parameters to train. Reached val accuracy of 57% and trainina accuracy of aroun 80%. \n",
    "\n",
    "## 6th Architecture\n",
    "Changing the target column to be Enlarged Cardiomediastinum as that was found to be easier to detect in previous papers. Doing this also allowed us to increase the number of samples we use to 30K from 10K which should help a lot. Also increased untrainable layers to 420. This reduces trainable parameters to 200K.  If this doesn't work then we would also like to try data augementation I our next architecture to create more data, which will help the model generalise. Results were 80% training accuracy, 51% val accuracy. Therefore it's not working very well. \n",
    "\n",
    "## 7th Architecture\n",
    "Adding data augmentation. This changes the images slightly to increase the number of samples. Changes include slight rotations, slight translations. The images were also constrained to only include the PA angle, this reduced the number of samples to 20000. The training accuracy was 70% and val accuracy maxed out at 51%.\n",
    "\n",
    "## 8th\n",
    "same as 7th but using no finding as the negative case to avoid confusion with other diseases. Seemed very unstable but achieve max val acc of 56% and training acc of 66%. \n",
    "\n",
    "## 9th\n",
    "Tried freezing all the densenet layers. Reduced batch size to 16 to match the checxpert paper. 50k parameters. trainign 70% and val 50%.\n",
    "\n",
    "## 10th\n",
    "After reading: https://arxiv.org/pdf/1711.05225.pdf it turns out our original architecutre was much more similar to their papaer which ad some success. To more closely match theirs, i will change the disease to pneumonia, have all layers trainable and also set batch size to 16 to match their paper. They allow flipping of the image, which i think is invalid so will keep our data augmentation techniques. 7 million parameters. The batch size of 16 means we have a lot of backpropagations but it's much slower as a result. \n",
    "Achieved a training accuracy of 65% and validation accuracy of 72%, which is strange but we can see the val acc is very unstable so perhaps more reasonable to say around 70% if we take the average of the last few. \n",
    "\n",
    "## 11th\n",
    "Same as 10th, but to deal with the instability I increaed the epochs to 100. I will then repeat this architecture for different diseases and see what we get. \n",
    "\n",
    "## 12th \n",
    "11th was going to take 5 hours and we weres till seeing instability after 35 epochs so reduced to 20 epochs to help us understnd the instability. We also tried to run with batchsie 64 to reduce instability we had seen but gpu mem was too small for this so kept it at 16. Note pneumonia scored 76% val acc and 65% training acc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trnsfer learning model\n",
    "# put into separate cell as getting the dense net takes time \n",
    "# and we often only want to tweak the downstream architecutre \n",
    "denseNet = DenseNet121(input_shape=(224,224,3), include_top=True)\n",
    "denseNet.layers.pop() # remov elast layer which has 1000 class softmax in it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2Layers = Flatten()(denseNet.layers[-2].output)\n",
    "model2Layers = Dropout(0.3)(model2Layers)\n",
    "model2Layers = Dense(1,activation='sigmoid')(model2Layers)\n",
    "model2 = Model(input=denseNet.layers[0].input, output=model2Layers)\n",
    "for i,layer in enumerate(model2.layers):\n",
    "    # Don't train the first layers to save mem and they wil be picking up low level features anyway. \n",
    "    if i < 428:\n",
    "        #layer.trainable=False\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()\n",
    "weightsFilePath2=\"weights2.best.hdf5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out to prevent retraining when doing analysis\n",
    "'''\n",
    "checkpoint2 = ModelCheckpoint(weightsFilePath2, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "image_gen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=.15,\n",
    "    height_shift_range=.15)\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "history2 = model2.fit_generator(image_gen.flow(x_train3Channel, y_train, batch_size=batch_size),steps_per_epoch=len(x_train3Channel) / batch_size,  epochs = epochs, validation_data=(x_val3Channel, y_val), callbacks=[checkpoint2])\n",
    "model2.load_weights(weightsFilePath2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out as we have done the training and don't need to rerun this now\n",
    "'''\n",
    "\n",
    "# Plot the history of this model\n",
    "plt.plot(history2.history['acc'])\n",
    "plt.plot(history2.history['val_acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Val'])\n",
    "maxValAcc = max(history2.history['val_acc'])\n",
    "trainAcc = history2.history['acc'][history2.history['val_acc'].index(maxValAcc)]\n",
    "plt.savefig(f\"Architecture10-{epochs}epochs-{colName}--trainAcc-{trainAcc}--valAcc-{maxValAcc}.png\")\n",
    "model2.save_weights(f\"Architecture10-{epochs}epochs-{colName}--trainAcc-{trainAcc}--valAcc-{maxValAcc}.hdf5\")\n",
    "plt.show() '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "In this section we analyse the results for each disease model. The models will load the weights from the best performing (as defined by validation accuracy) epoch. We then produce confusion matrices to calculate recall, precision and f1score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "testDf = pd.read_csv('CheXpert-v1.0-small/valid.csv')\n",
    "\n",
    "# Remove anomalous dataline\n",
    "testDf = testDf[testDf.Sex != 'Unknown']\n",
    "# Drop this column as it has many more classifications than lit suggests and shouldn't matter greatly for a CNN\n",
    "\n",
    "def pathToID(path):\n",
    "    pathList = path.split('/')\n",
    "    return pathList[2][7:]\n",
    "\n",
    "def pathToStudy(path):\n",
    "    pathList = path.split('/')\n",
    "    return pathList[3][5:]\n",
    "\n",
    "# Convert all labels to a series of one-hot encoded labels. \n",
    "# -1 is uncertain, 0 is negative, 1 is positive, nans are no mention of the disease in the text\n",
    "testDf = testDf.fillna(0)\n",
    "# N.B. this is replacing unknowns with true as per u-ones model here: https://arxiv.org/pdf/1901.07031.pdf\n",
    "# This is essentialyl saying that if we're not sure of disease we say they have it. \n",
    "# Just to be on the safeside and have better recall as we care more about recall than precision\n",
    "testDf = testDf.replace(-1,1) \n",
    "\n",
    "\n",
    "# Onehot encode the sex and the xray orientation\n",
    "testDf = testDf.replace('Male',1)\n",
    "testDf = testDf.replace('Female',0)\n",
    "testDf = testDf.replace('Frontal',1)\n",
    "testDf = testDf.replace('Lateral',0)\n",
    "\n",
    "testDf =testDf.rename(index=str, columns={\"Sex\": \"Male?\",'Frontal/Lateral' :'Frontal1/Lateral0'})\n",
    "\n",
    "\n",
    "#trainDf.insert(0,'Path', trainDf['Path'])\n",
    "testDf['Study'] = testDf.Path.apply(pathToStudy)\n",
    "testDf['Patient ID'] = testDf.Path.apply(pathToID)\n",
    "\n",
    "# Rearrange Columns\n",
    "cols = ['Patient ID', 'Study', 'Path', 'Age', 'Male?', 'Frontal1/Lateral0', 'AP/PA','No Finding',\n",
    "       'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "       'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "       'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "       'Support Devices']\n",
    "testDf = testDf[cols]\n",
    "\n",
    "x_test3Channel = paths_to_tensor_channel_last_3colour(testDf.Path,inputSize)#.astype('float32')/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyseResults(model,x_test3Channel, testDf, disease, radiologistScores=None):\n",
    "    # Identify correct weights file to load\n",
    "    file = [f for f in os.listdir('.') if os.path.isfile(f) and f\"20epochs-{disease}\"  in f and \".hdf5\" in f]\n",
    "    if len(file) != 1:\n",
    "        print(f\"Error: can't find single weights file, instead found: {file}\")\n",
    "    model.load_weights(file[0])\n",
    "    model_predict_output = model.predict(x_test3Channel).flatten()\n",
    "    y_test = testDf[disease]\n",
    "    fpr, tpr, _ = roc_curve(y_test, model_predict_output)\n",
    "    auroc = roc_auc_score(y_test,model_predict_output)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr)\n",
    "    if radiologistScores != None:  \n",
    "        print(radiologistScores[0])\n",
    "        plt.plot(radiologistScores[0][0],radiologistScores[0][1],'ro', label=\"Rad1\") \n",
    "        plt.plot(radiologistScores[1][0],radiologistScores[1][1],'bo', label=\"Rad1\") \n",
    "        plt.plot(radiologistScores[2][0],radiologistScores[2][1],'go', label=\"Rad1\") \n",
    "\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    #plt.title(f'{disease} ROC curve')\n",
    "    plt.savefig(f\"Architecture10-{disease}--AUROC-{auroc}.png\")\n",
    "\n",
    "    plt.show()\n",
    "    return auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = ['Cardiomegaly','Atelectasis','Edema','Pneumonia','Pneumothorax']\n",
    "rads = [  [[0.05,0.48],[0.23,0.85],[0.11,0.70]], [[0.21,0.8],[0.18,0.71],[0.31,0.92]], [[0.09,0.63],[0.19,0.79],[0.07,0.58]] ]\n",
    "for i,disease in enumerate(diseases):\n",
    "    if i <= 2:\n",
    "        auroc = analyseResults(model2,x_test3Channel,testDf,disease, radiologistScores=rads[i])\n",
    "    else:\n",
    "        auroc = analyseResults(model2,x_test3Channel,testDf,disease)\n",
    "    print(f\"{disease} AUROC: {auroc}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
